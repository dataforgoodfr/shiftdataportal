{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EIA International\n",
    "\n",
    "This notebook : \n",
    "- downloads the bulk file on EIA open data portal\n",
    "- opens the txt file containing JSONs and convert it into a DataFrame using pandas\n",
    "- clean the data and structure it the way we want\n",
    "- then, store it into a csv file named \"../../data/_processed/eia_international_processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json\n",
    "import zipfile\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download du fichier international .zip sur eia (https://www.eia.gov/opendata/)\n",
    "\n",
    "# url of the international file on eia\n",
    "url = \"https://www.eia.gov/opendata/bulk/INTL.zip\"\n",
    "\n",
    "\n",
    "#/Users/alexandrebernard/Documents/Perso/Data/D4G/shiftdataportal_data\n",
    "#defining file name and destination_raw\n",
    "destination_raw = \"../../data/_raw/eia/\"\n",
    "destination_processed = \"../../data/_processed/\" \n",
    "file_name = \"eia_international_bulk.zip\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Download of the txt file containing all data of International API route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download completed\n"
     ]
    }
   ],
   "source": [
    "#download the file from url\n",
    "urllib.request.urlretrieve(url, destination_raw + file_name)\n",
    "print(\"download completed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unzip the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file unzipped\n"
     ]
    }
   ],
   "source": [
    "# unzip the file and delete zip\n",
    "\n",
    "#ouvrir le fichier en mode lecture\n",
    "with zipfile.ZipFile(destination_raw + file_name, 'r') as zip_ref:\n",
    "    #Extract file in same repertory\n",
    "    zip_ref.extractall(destination_raw)\n",
    "print(\"file unzipped\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of the JSONs from txt file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---open txt file---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---iteration on 5000 row completed---\n",
      "---iteration on 10000 row completed---\n",
      "---iteration on 15000 row completed---\n",
      "---iteration on 20000 row completed---\n",
      "---iteration on 25000 row completed---\n",
      "---iteration on 30000 row completed---\n",
      "---iteration on 35000 row completed---\n",
      "---iteration on 40000 row completed---\n",
      "---iteration on 45000 row completed---\n",
      "---iteration on 50000 row completed---\n",
      "---iteration on 55000 row completed---\n",
      "---iteration on 60000 row completed---\n",
      "---iteration on 65000 row completed---\n",
      "---iteration on 70000 row completed---\n",
      "---iteration on 75000 row completed---\n",
      "---iteration on 80000 row completed---\n",
      "---iteration on 85000 row completed---\n",
      "---iteration on 90000 row completed---\n",
      "---iteration on 95000 row completed---\n",
      "---iteration on 100000 row completed---\n",
      "--- iteration done in 19.729034900665283 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "json_list = []\n",
    "\n",
    "print(\"---open txt file---\")\n",
    "#open the file in reading mode\n",
    "with open(destination_raw + \"INTL.txt\", 'r') as file_txt:\n",
    "\n",
    "    count = 0                        # TO DELETE\n",
    "    # iteration on each json in the txt file\n",
    "    for row in file_txt:\n",
    "        #delete blank and new line caracter\n",
    "        row = row.strip()\n",
    "        #load the txt of one row into a json. use of \"json.loads()\" as we are loading a string into a json file corresponding to the row\n",
    "        data_json = json.loads(row)\n",
    "        #insert in dataframe\n",
    "        #df_temp = pd.DataFrame(data_json)\n",
    "        #add to the list of jsons\n",
    "        json_list.append(data_json)\n",
    "        \n",
    "        #management of execution time\n",
    "        count += 1                   # TO DELETE\n",
    "        if count % 5000 == 0:\n",
    "            print(\"---iteration on %s row completed---\" %count)\n",
    "        #if stop == 20:              # TO DELETE\n",
    "        #    break                   # TO DELETE\n",
    "\n",
    "df = pd.DataFrame(json_list)\n",
    "\n",
    "iteration_time = time.time()\n",
    "print(\"--- iteration done in %s seconds ---\" %(iteration_time - start_time))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletion of NaN values\n",
    "df_structured = df.dropna(subset=[\"data\"])\n",
    "\n",
    "#we only keep annual data\n",
    "df_structured = df_structured[df_structured[\"f\"]==\"A\"]\n",
    "\n",
    "#management of the columns \"data\" that is a list of lists\n",
    "df_structured = df_structured.explode(\"data\")\n",
    "\n",
    "# transformation of the \"data\" columns (containing a list [year, value] into 2 columns for date and value\n",
    "df_structured[[\"date\", \"value\"]] = pd.DataFrame(df_structured[\"data\"].tolist(), index=df_structured.index)\n",
    "df_structured = df_structured.drop(\"data\", axis=1)\n",
    "\n",
    "#split of concatenated columns for the \"series_id\" column\n",
    "df_structured[[\"file_name\", \"product_id\", \"activity_id\", \"country_region_id\", \"unit_id\", \"frequency_id\"]] = df_structured[\"series_id\"].str.split('[.-]', expand=True)\n",
    "# \"name\" column not working because there can be 2 until 7 \",\"\n",
    "\n",
    "#we do not keep regional data.\n",
    "df_structured = df_structured[df_structured[\"country_region_id\"].str.len()!=4]\n",
    "#df_structured.shape (3 952 091, 23) puis (3 364 396) aprÃ¨s suppr des regions\n",
    "# Warning : Following countrie have 4 caracter for there id : DEUW allemagne ouest, DEUO GM offshore used for load only, \n",
    "# GBRO UK offshore used for load only, NLDA Netherlands antilles, NLDO NL offshore used for load only\n",
    "# USIQ US pacific islands, USOH us territories\n",
    "\n",
    "df_structured = df_structured.drop_duplicates([\"name\", \"country_region_id\", \"activity_id\", \"date\", \"unit_id\"])\n",
    "#=> shape of the df before droping duplicates : (3364396, 11). After deleting duplicates : (3345590, 11)\n",
    "\n",
    "#reset of indexes\n",
    "df_structured = df_structured.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns renaming and selection, value column cleaning, country in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/8mtfl9m11sbf2r8nhg8s0jc80000gp/T/ipykernel_19070/1090340945.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_eia_cleaned.rename(columns={\n",
      "/var/folders/pd/8mtfl9m11sbf2r8nhg8s0jc80000gp/T/ipykernel_19070/1090340945.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_eia_cleaned.loc[:,\"value\"] = df_eia_cleaned[\"value\"].replace(values_to_replace, 0)\n",
      "/var/folders/pd/8mtfl9m11sbf2r8nhg8s0jc80000gp/T/ipykernel_19070/1090340945.py:15: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_eia_cleaned.loc[:,\"value\"] = df_eia_cleaned[\"value\"].replace(values_to_replace, 0)\n",
      "/var/folders/pd/8mtfl9m11sbf2r8nhg8s0jc80000gp/T/ipykernel_19070/1090340945.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_eia_cleaned.loc[:,\"value\"] = df_eia_cleaned[\"value\"].astype(float)\n",
      "/var/folders/pd/8mtfl9m11sbf2r8nhg8s0jc80000gp/T/ipykernel_19070/1090340945.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_eia_cleaned.loc[:,\"country_id\"] = df_eia_cleaned[\"country_id\"].str.lower()\n",
      "/var/folders/pd/8mtfl9m11sbf2r8nhg8s0jc80000gp/T/ipykernel_19070/1090340945.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_eia_cleaned.loc[:,(\"product_id\", \"activity_id\")] = df_eia_cleaned[[\"product_id\", \"activity_id\"]].astype(int)\n",
      "/var/folders/pd/8mtfl9m11sbf2r8nhg8s0jc80000gp/T/ipykernel_19070/1090340945.py:24: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_eia_cleaned.loc[:,(\"product_id\", \"activity_id\")] = df_eia_cleaned[[\"product_id\", \"activity_id\"]].astype(int)\n"
     ]
    }
   ],
   "source": [
    "#selection and reordering of the columns\n",
    "df_eia_cleaned = df_structured[[\"source\", \"file_name\", \"name\", \"country_region_id\", \"product_id\", \"activity_id\", \"date\", \"unit_id\", \"units\", \"value\", \"last_updated\"]]\n",
    "\n",
    "#colums renaming\n",
    "df_eia_cleaned.rename(columns={\n",
    "    'units': 'unit_name',\n",
    "    'name': 'product_region_freq_name',\n",
    "    'date': 'year',\n",
    "    'country_region_id': 'country_id'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "#clean value column\n",
    "values_to_replace = [\"--\", \"ie\", \"w\", \"NA\"]\n",
    "df_eia_cleaned.loc[:,\"value\"] = df_eia_cleaned[\"value\"].replace(values_to_replace, 0)\n",
    "\n",
    "#conversion of value into float data type\n",
    "df_eia_cleaned.loc[:,\"value\"] = df_eia_cleaned[\"value\"].astype(float)\n",
    "\n",
    "#conversion of value into float data type\n",
    "df_eia_cleaned.loc[:,\"country_id\"] = df_eia_cleaned[\"country_id\"].str.lower()\n",
    "\n",
    "#conversion of product_id into int data type\n",
    "df_eia_cleaned.loc[:,(\"product_id\", \"activity_id\")] = df_eia_cleaned[[\"product_id\", \"activity_id\"]].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add external data (product name, activity name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load of the product names\n",
    "#data (product_name, product_id) come from a copy paste on the API browser\n",
    "#this data processed in this notebook : notebooks/raw/ab_eia_raw_product_name_id.ipynb\n",
    "df_product = pd.read_csv(\"../../data/_processed/eia_product_name_id.csv\")\n",
    "\n",
    "# load of activity names\n",
    "# As it's complicated to extract activity text, let's build the data from data in the API exploration browser\n",
    "df_activity = pd.DataFrame({\"activity_id\" : [1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 23, 33, 34],\n",
    "                            \"activity_name\" : [\"Production\", \"Consumption\", \"Imports\", \"Exports\", \"Stocks, OECD\", \"Reserves\", \"Capacity\", \"Emissions\", \"Distribution losses\", \"Generation\", \"Bunker\", \"Net Imports\", \"Population\", \"GDP\"]})\n",
    "\n",
    "# load of the countries\n",
    "df_country = pd.read_csv(\"../../data/countries.csv\")\n",
    "\n",
    "# merge of the product names\n",
    "df_eia = df_eia_cleaned.merge(df_product, left_on=\"product_id\", right_on=\"product_id\", how=\"left\")\n",
    "\n",
    "# merge of the activity names\n",
    "df_eia = df_eia.merge(df_activity, left_on=\"activity_id\", right_on=\"activity_id\", how=\"left\")\n",
    "\n",
    "# merge of the countries\n",
    "df_eia = df_eia.merge(df_country[[\"alpha3\", \"name\"]], left_on=\"country_id\", right_on=\"alpha3\", how=\"left\")\n",
    "#country name colums renaming and drop of alpha3 column\n",
    "df_eia.rename(columns={'name': 'country_name'}, inplace=True)\n",
    "df_eia.drop(\"alpha3\", axis=1)\n",
    "\n",
    "#columns reordering\n",
    "df_eia = df_eia[['source', 'file_name', 'product_region_freq_name', 'country_id',\n",
    "                 'country_name','product_id', 'product_name', 'activity_id', 'activity_name'\n",
    "                 , 'year', 'unit_id', 'unit_name', 'value','last_updated']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store cleaned eia data in csv file + delete zip and unzipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index on false to not send it into the csv file\n",
    "df_eia.to_csv(destination_processed + \"eia_international_processed.csv\", index=False)\n",
    "\n",
    "os.remove(destination_raw + \"eia_international_bulk.zip\")\n",
    "os.remove(destination_raw + \"INTL.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
